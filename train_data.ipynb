{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63704061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer as wnl\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e738a91f",
   "metadata": {},
   "source": [
    "Lemmatization: It is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "568ec162",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = wnl()  \n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "with open('intents.json', \"r\") as data_file:\n",
    "    intents = json.load(data_file)\n",
    "\n",
    "# Preprocessing the data:\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "\n",
    "        documents.append((w, intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1352365",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[:5])\n",
    "print(classes[:6])\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00d94254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the words together according to wordNet:\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "# Removing the duplicates and sorting them:\n",
    "words = sorted(list(set(words)))\n",
    "classes = sorted(list(set(classes)))\n",
    "with open('words.pkl', 'wb') as fd1:\n",
    "    pickle.dump(words, fd1) \n",
    "with open('classes.pkl', 'wb') as fd2:\n",
    "    pickle.dump(classes, fd2) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7c25d74",
   "metadata": {},
   "source": [
    "splitting the data for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "381e9351",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "# create an empty list with with default values 0 \n",
    "output = [0]*len(classes)\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    pattern_words = doc[0]\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    \n",
    "    # create a bag to represent the words found as 1 else 0 \n",
    "    # and an output_row with the corresponding tag as set to 1 for training \n",
    "    for w in words:    \n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "        output_row = list(output)\n",
    "        output_row[classes.index(doc[1])] = 1\n",
    "        training.append([bag, output_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe9288",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(training[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea645b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "trainX = list(training[:,0])\n",
    "trainY = list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb69f05a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(trainX[0:1])\n",
    "print(trainY[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3b128c",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "20269a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128,input_shape=(len(trainX[0]),),activation ='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(trainY[0]), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e55ceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(learning_rate=0.01,decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3cf06f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MOD = model.fit(np.array(trainX), np.array(trainY), epochs=200, batch_size=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aa613182",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\", MOD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
